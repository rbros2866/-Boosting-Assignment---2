{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a popular machine learning technique used for regression tasks. It belongs to the family of ensemble learning methods, which combine multiple individual models to create a more powerful predictive model.\n",
    "\n",
    "In gradient boosting regression, decision trees are typically used as the base learners. The algorithm works by sequentially adding decision trees to an ensemble, where each tree corrects the errors made by the previous one.\n",
    "\n",
    "**Initialization:** The model starts with an initial prediction (usually the mean of the target variable for regression tasks).\n",
    "\n",
    "**Sequential Learning:** Iteratively, new decision trees are added to the ensemble, each one trained to predict the residual errors (the difference between the actual target values and the predictions made by the current ensemble).\n",
    "\n",
    "**Gradient Descent Optimization:** At each step, the new tree is fitted to the negative gradient of the loss function with respect to the current ensemble's predictions. This helps minimize the loss when adding the new tree to the ensemble.\n",
    "\n",
    "**Shrinkage:** To prevent overfitting, a shrinkage parameter (learning rate) is introduced, which scales the contribution of each tree added to the ensemble.\n",
    "\n",
    "**Final Prediction:** The final prediction is made by summing the predictions of all the trees in the ensemble, weighted by the shrinkage parameter.\n",
    "\n",
    "Gradient boosting regression is known for its ability to handle complex nonlinear relationships in data and typically performs very well in practice, often achieving state-of-the-art performance on a wide range of regression tasks. Popular implementations of gradient boosting regression include XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Define the loss function (mean squared error).\n",
    "\n",
    "**2.** Initialize the model with some parameters (number of trees, learning rate, etc.).\n",
    "\n",
    "**3.** For each tree:\n",
    "\n",
    "**a.** Compute the negative gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "**b.** Fit a decision tree to the negative gradient.\n",
    "\n",
    "**c.** Update the predictions by adding the predictions of the new tree scaled by the learning rate.\n",
    "\n",
    "**4.** Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute negative gradient\n",
    "            gradient = y - predictions\n",
    "            \n",
    "            # Fit a decision tree to the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, gradient)\n",
    "            \n",
    "            # Update predictions by adding the predictions of the new tree scaled by learning rate\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the trained tree\n",
    "            self.models.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 88.39630695181542\n",
      "R-squared: -1.9393015233758137\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Example usage\n",
    "# Generating some synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100)  # y = 2x + noise\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Creating and training the model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.05011681487615215, 'max_depth': 3, 'n_estimators': 107}\n",
      "Mean Squared Error: 1.1455019574292524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees in the forest\n",
    "    'learning_rate': uniform(0.05, 0.15),  # Learning rate for the boosting process\n",
    "    'max_depth': randint(3, 6)  # Maximum depth of the individual regression estimators\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressor object\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a base model or a simple predictive model that performs only slightly better than random guessing on a given problem. Specifically, it refers to models that are relatively simple and have limited predictive power on their own.\n",
    "\n",
    "Typically, decision trees with shallow depth (also known as decision stumps) are used as weak learners in Gradient Boosting algorithms. These decision trees have a limited number of splits and are often constrained in depth, which makes them weak learners. Despite their simplicity, when combined with the boosting technique, they can collectively form a strong predictive model.\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to sequentially train weak learners in a stage-wise fashion, where each new weak learner is trained to correct the errors made by the combination of all previous weak learners. By iteratively focusing on the difficult-to-predict instances, Gradient Boosting builds a strong ensemble model by combining the predictions of multiple weak learners. This approach allows for the creation of highly accurate predictive models, even when using simple weak learners.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q5.** What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential Learning:** Gradient Boosting is an ensemble learning technique that works by sequentially adding predictors to an ensemble, with each predictor correcting its predecessor's errors.\n",
    "\n",
    "**Weak Learners:** It uses a sequence of weak learners, typically decision trees with shallow depth, as base models. These weak learners are combined to form a strong ensemble model.\n",
    "\n",
    "**Gradient Descent:** The term \"gradient\" in Gradient Boosting refers to the gradient descent optimization algorithm used to minimize the loss function. At each stage of training, the algorithm calculates the gradient of the loss function with respect to the current ensemble's predictions. It then fits a weak learner to the gradient in order to minimize the residual errors.\n",
    "\n",
    "**Gradient-Based Weighting:** The algorithm assigns weights to each weak learner based on their performance in reducing the loss function. Weak learners that contribute more to reducing the errors are given higher weights, while those that contribute less are given lower weights.\n",
    "\n",
    "**Boosting:** The term \"boosting\" refers to the sequential nature of the algorithm, where each weak learner is trained to improve upon the mistakes of its predecessors. By iteratively adding predictors to the ensemble, the model gradually reduces the residual errors, leading to a more accurate overall prediction.\n",
    "\n",
    "**Combining Predictions:** Finally, the predictions of all weak learners are combined to make the final prediction of the ensemble model. This combination is usually done by taking a weighted sum of the predictions, where the weights are determined by the performance of each weak learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization:** The process begins by initializing the ensemble with a simple model, often just the mean of the target variable for regression problems or the majority class for classification problems.\n",
    "\n",
    "**Sequential Training:** In each iteration of the algorithm, a new weak learner (typically a decision tree with shallow depth) is trained on the dataset. The goal of this weak learner is to correct the errors made by the ensemble of models built in the previous iterations.\n",
    "\n",
    "**Gradient Calculation:** The algorithm calculates the gradient of the loss function with respect to the predictions of the current ensemble. This gradient represents the direction in which the predictions need to be adjusted in order to minimize the loss function.\n",
    "\n",
    "**Fitting the Weak Learner:** The weak learner is then trained to predict the negative gradient. Essentially, the weak learner is fitted to the residual errors of the current ensemble, aiming to reduce these errors further.\n",
    "\n",
    "**Adding to Ensemble:** Once the weak learner is trained, its predictions are combined with the predictions of the existing ensemble models. This combination is typically done by adding a scaled version of the weak learner's predictions to the ensemble.\n",
    "\n",
    "**Weighting:** The contribution of each weak learner to the ensemble is determined based on its performance in reducing the loss function. Weak learners that contribute more to reducing the errors are given higher weights, while those that contribute less are given lower weights.\n",
    "\n",
    "**Iterative Process:** Steps 3 to 6 are repeated for a fixed number of iterations or until a certain stopping criterion is met (e.g., when the loss function no longer improves).\n",
    "\n",
    "**Final Ensemble Prediction:** The final prediction of the ensemble model is obtained by combining the predictions of all weak learners in the ensemble, typically by taking a weighted sum of their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the underlying principles of gradient descent optimization and the additive nature of the ensemble model. Here are the key steps involved:\n",
    "\n",
    "**Loss Function:** Define a differentiable loss function that measures the difference between the predicted values and the true values of the target variable. Common loss functions include mean squared error for regression problems and cross-entropy loss for classification problems.\n",
    "\n",
    "**Initial Prediction:** Start with an initial prediction, often a simple one like the mean of the target variable for regression problems or the log-odds for classification problems.\n",
    "\n",
    "**Gradient Calculation:** Compute the gradient of the loss function with respect to the initial prediction. This gradient represents the direction and magnitude of the error, indicating how much the prediction needs to be adjusted to minimize the loss.\n",
    "\n",
    "**Weak Learner Fitting:** Train a weak learner (e.g., decision tree) to predict the negative gradient. Essentially, fit the weak learner to the residual errors of the initial prediction.\n",
    "\n",
    "**Additive Modeling:** Add the weak learner's predictions to the initial prediction, adjusting the prediction in the direction indicated by the gradient. This creates a new, slightly improved prediction.\n",
    "\n",
    "**Weighting:** Weight the contribution of the weak learner to the ensemble based on its performance in reducing the loss function. Weak learners that contribute more to reducing the errors are given higher weights.\n",
    "\n",
    "**Iteration:** Repeat steps 3 to 6 iteratively, with each iteration introducing a new weak learner to correct the errors made by the ensemble of models built in the previous iterations.\n",
    "\n",
    "**Stopping Criterion:** Decide on a stopping criterion, such as a maximum number of iterations or reaching a minimum improvement in the loss function.\n",
    "\n",
    "**Final Ensemble Prediction:** Combine the predictions of all weak learners in the ensemble to make the final prediction. This combination is typically done by taking a weighted sum of their predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
